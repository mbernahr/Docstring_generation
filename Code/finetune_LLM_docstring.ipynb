{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "zXgtx19JcHOP",
        "outputId": "64b283f6-dfa0-473a-f20f-bf8d8ab86f94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.3.0)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.14.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.45.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.48.1)\n",
            "Requirement already satisfied: trl in /usr/local/lib/python3.10/dist-packages (0.13.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.19.4)\n",
            "Requirement already satisfied: mlflow in /usr/local/lib/python3.10/dist-packages (2.19.0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (7.2.3)\n",
            "Requirement already satisfied: numpy==1.24.3 in /usr/local/lib/python3.10/dist-packages (1.24.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.5.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.27.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.67.1)\n",
            "Requirement already satisfied: typing_extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from trl) (13.9.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.25.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.10.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.19.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: mlflow-skinny==2.19.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.19.0)\n",
            "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.1.0)\n",
            "Requirement already satisfied: Jinja2<4,>=2.11 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.1.4)\n",
            "Requirement already satisfied: alembic!=1.10.0,<2 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.14.1)\n",
            "Requirement already satisfied: docker<8,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (7.1.0)\n",
            "Requirement already satisfied: graphene<4 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.4.3)\n",
            "Requirement already satisfied: gunicorn<24 in /usr/local/lib/python3.10/dist-packages (from mlflow) (23.0.0)\n",
            "Requirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.7)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.8.0)\n",
            "Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.6.0)\n",
            "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.13.1)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.0.36)\n",
            "Requirement already satisfied: cachetools<6,>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.19.0->mlflow) (5.5.0)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.19.0->mlflow) (3.1.0)\n",
            "Requirement already satisfied: databricks-sdk<1,>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.19.0->mlflow) (0.41.0)\n",
            "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.19.0->mlflow) (8.5.0)\n",
            "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.19.0->mlflow) (1.29.0)\n",
            "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.19.0->mlflow) (1.29.0)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.19.0->mlflow) (0.5.3)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic!=1.10.0,<2->mlflow) (1.3.8)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from docker<8,>=4.0.0->mlflow) (2.2.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.10/dist-packages (from Flask<4->mlflow) (3.1.3)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.10/dist-packages (from Flask<4->mlflow) (2.2.0)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.10/dist-packages (from Flask<4->mlflow) (1.9.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: graphql-core<3.3,>=3.1 in /usr/local/lib/python3.10/dist-packages (from graphene<4->mlflow) (3.2.5)\n",
            "Requirement already satisfied: graphql-relay<3.3,>=3.1 in /usr/local/lib/python3.10/dist-packages (from graphene<4->mlflow) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from graphene<4->mlflow) (2.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2<4,>=2.11->mlflow) (3.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (3.2.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2->mlflow) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2->mlflow) (3.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.1.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (2.18.0)\n",
            "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.10/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.19.0->mlflow) (2.27.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==2.19.0->mlflow) (3.21.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.19.0->mlflow) (1.2.15)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.50b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.19.0->mlflow) (0.50b0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.19.0->mlflow) (1.17.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.19.0->mlflow) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.19.0->mlflow) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.19.0->mlflow) (0.6.1)\n"
          ]
        }
      ],
      "source": [
        "# Configuring the character encoding\n",
        "import locale\n",
        "\n",
        "\n",
        "def getpreferredencoding(do_setlocale=True):\n",
        "    return \"UTF-8\"\n",
        "\n",
        "\n",
        "locale.getpreferredencoding = getpreferredencoding\n",
        "\n",
        "!pip install -U accelerate peft bitsandbytes transformers trl datasets wandb mlflow python-dotenv pyngrok numpy==1.24.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Cuv3We8VcHOU"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import torch\n",
        "import wandb\n",
        "import numpy\n",
        "import random\n",
        "import mlflow\n",
        "import hashlib\n",
        "from dotenv import load_dotenv\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        ")\n",
        "from datasets import Dataset, DatasetDict, Features, Value\n",
        "from peft import LoraConfig, PeftModel\n",
        "from trl import SFTTrainer\n",
        "from pyngrok import ngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sw0ooda1gitd",
        "outputId": "1ba9c4ce-058c-40ea-e54d-46662795f0bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Jan 21 17:01:44 2025       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P0              43W / 400W |      2MiB / 40960MiB |      0%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZqceolScHOV",
        "outputId": "ad52ba09-59a8-4e48-eb49-becb1783eb9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "The notebook runs on Google Colab.\n",
            "Access token loaded.\n"
          ]
        }
      ],
      "source": [
        "# Check if the notebook runs on Colab to adjust paths\n",
        "on_colab = 'google.colab' in sys.modules\n",
        "\n",
        "if on_colab:\n",
        "    from google.colab import drive, userdata\n",
        "    drive.mount('/content/drive')\n",
        "    test_file = \"/content/drive/MyDrive/Data/docstring_test_data.txt\"\n",
        "    train_file = \"/content/drive/MyDrive/Data/docstring_training_data.txt\"\n",
        "    base_output_dir = \"/content/drive/MyDrive/Models/\"\n",
        "    print(\"The notebook runs on Google Colab.\")\n",
        "\n",
        "    # Load API-Token from Colab-Secrets\n",
        "    huggingface_api_token = userdata.get('huggingface_api_token')\n",
        "    wandb_api_token = userdata.get('wandb_api_token')\n",
        "    ngrok_api_token = userdata.get('ngrok_api_token')\n",
        "else:\n",
        "    test_file = \"../Data/docstring_test_data.txt\"\n",
        "    train_file = \"../Data/docstring_training_data.txt\"\n",
        "    base_output_dir = \"../Models/\"\n",
        "    print(\"The notebook is running locally.\")\n",
        "\n",
        "    # Load API-Token from .env\n",
        "    load_dotenv()\n",
        "    huggingface_api_token = os.getenv(\"HUGGINGFACE\")\n",
        "    wandb_api_token = os.getenv(\"WANDB\")\n",
        "    ngrok_api_token = os.getenv('NGROK')\n",
        "\n",
        "if huggingface_api_token and wandb_api_token and ngrok_api_token:\n",
        "    print(\"Access token loaded.\")\n",
        "else:\n",
        "    print(\"Access token not found.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPVMHDjrcHOW",
        "outputId": "c2992f0e-9981-4a03-e23c-60cb08f34181"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEST (File size 6673607 bytes):\n",
            "[Function] def shortest_dist(dist_mat):\\n    (m, n) = dist_mat.size()[:2]\\n    dist = [[0 for _ in range(n)] for _ in range(m)]\\n    for i in range(m):\\n        for j in range(n):\\n            if i == 0 and j == 0:\\n                dist[i][j] = dist_mat[i, j]\\n            elif i == 0 and j > 0:\\n                dist[i][j] = dist[i][j - 1] + dist_mat[i, j]\\n            elif i > 0 and j == 0:\\n                dist[i][j] = dist[i - 1][j] + dist_mat[i, j]\\n            else:\\n                dist[i][j] = torch.min(dist[i - 1][j], dist[i][j - 1]) + dist_mat[i, j]\\n    dist = dist[-1][-1]\\n    return dist [Docstring] Parallel version.\\nArgs:\\n  dist_mat: pytorch Variable, available shape:\\n    1) [m, n]\\n    2) [m, n, N], N is batch size\\n    3) [m, n, *], * can be arbitrary additional dimensions\\nReturns:\\n  dist: three cases corresponding to `dist_mat`:\\n    1) scalar\\n    2) pytorch Variable, with shape [N]\\n    3) pytorch Variable, with shape [*] [EOS]\n",
            "\n",
            "[Function] def _construct_linear_audio_network(include_frontend=True):\\n    weight_decay = 1e-05\\n    n_dft = 512\\n    n_hop = 242\\n    asr = 48000\\n    audio_window_dur = 1\\n    if include_frontend:\\n        input_shape = (1, asr * audio_window_dur)\\n        x_a = Input(shape=input_shape, dtype='float32')\\n        from kapre.composed import get_stft_magnitude_layer\\n        spec = __fix_kapre_spec(get_stft_magnitude_layer)(input_shape=input_shape, n_fft=n_dft, hop_length=n_hop, return_decibel=True, input_data_format='channels_first', output_data_format='channels_last')\\n        y_a = spec(x_a)\\n    else:\\n        input_shape = (n_dft // 2 + 1, int(np.ceil((asr - n_dft) * audio_window_dur / n_hop)), 1)\\n        x_a = y_a = Input(shape=input_shape, dtype='float32')\\n    y_a = BatchNormalization()(y_a)\\n    n_filter_a_1 = 64\\n    filt_size_a_1 = (3, 3)\\n    pool_size_a_1 = (2, 2)\\n    y_a = Conv2D(n_filter_a_1, filt_size_a_1, padding='same', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay))(y_a)\\n    y_a = BatchNormalization()(y_a)\\n    y_a = Activation('relu')(y_a)\\n    y_a = Conv2D(n_filter_a_1, filt_size_a_1, padding='same', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay))(y_a)\\n    y_a = BatchNormalization()(y_a)\\n    y_a = Activation('relu')(y_a)\\n    y_a = MaxPooling2D(pool_size=pool_size_a_1, strides=2)(y_a)\\n    n_filter_a_2 = 128\\n    filt_size_a_2 = (3, 3)\\n    pool_size_a_2 = (2, 2)\\n    y_a = Conv2D(n_filter_a_2, filt_size_a_2, padding='same', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay))(y_a)\\n    y_a = BatchNormalization()(y_a)\\n    y_a = Activation('relu')(y_a)\\n    y_a = Conv2D(n_filter_a_2, filt_size_a_2, padding='same', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay))(y_a)\\n    y_a = BatchNormalization()(y_a)\\n    y_a = Activation('relu')(y_a)\\n    y_a = MaxPooling2D(pool_size=pool_size_a_2, strides=2)(y_a)\\n    n_filter_a_3 = 256\\n    filt_size_a_3 = (3, 3)\\n    pool_size_a_3 = (2, 2)\\n    y_a = Conv2D(n_filter_a_3, filt_size_a_3, padding='same', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay))(y_a)\\n    y_a = BatchNormalization()(y_a)\\n    y_a = Activation('relu')(y_a)\\n    y_a = Conv2D(n_filter_a_3, filt_size_a_3, padding='same', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay))(y_a)\\n    y_a = BatchNormalization()(y_a)\\n    y_a = Activation('relu')(y_a)\\n    y_a = MaxPooling2D(pool_size=pool_size_a_3, strides=2)(y_a)\\n    n_filter_a_4 = 512\\n    filt_size_a_4 = (3, 3)\\n    y_a = Conv2D(n_filter_a_4, filt_size_a_4, padding='same', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay))(y_a)\\n    y_a = BatchNormalization()(y_a)\\n    y_a = Activation('relu')(y_a)\\n    y_a = Conv2D(n_filter_a_4, filt_size_a_4, kernel_initializer='he_normal', name='audio_embedding_layer', padding='same', kernel_regularizer=regularizers.l2(weight_decay))(y_a)\\n    m = Model(inputs=x_a, outputs=y_a)\\n    return m [Docstring] Returns an uninitialized model object for an audio network with a linear\\nspectrogram input (With 257 frequency bins)\\n\\nReturns\\n-------\\nmodel : tf.keras.Model\\n    Model object. [EOS]\n",
            "\n",
            "[Function] def _expect_true(self, response):\\n    if response.text == u'true':\\n        return True\\n    raise BitstampError('Unexpected response') [Docstring] A shortcut that raises a :class:`BitstampError` if the response didn't\\njust contain the text 'true'. [EOS]\n",
            "\n",
            "TRAIN (File size 15639721 bytes):\n",
            "[Function] def squeeze_and_excite(inputs, filters, ratio, num_frames):\\n    reduced_filters = max(int(filters * ratio), 8)\\n    feature_shape = [inputs.shape[0], 1, 1, inputs.shape[3]]\\n    squeeze_excite_inputs = tf.reshape(inputs, [int(tf.compat.dimension_value(inputs.shape[0]) // num_frames), num_frames * inputs.shape[1], inputs.shape[2], -1])\\n    squeeze_excite = tf.reduce_mean(squeeze_excite_inputs, [1, 2], keepdims=True)\\n    squeeze_excite = tf.layers.conv2d(inputs=squeeze_excite, filters=reduced_filters, kernel_size=1, strides=1, padding='SAME', use_bias=True, kernel_initializer=contrib_layers.variance_scaling_initializer(factor=2.0, mode='FAN_IN', uniform=False), data_format='channels_last')\\n    squeeze_excite = tf.layers.conv2d(inputs=tf.nn.relu(squeeze_excite), filters=filters, kernel_size=1, strides=1, padding='SAME', use_bias=True, kernel_initializer=contrib_layers.variance_scaling_initializer(factor=2.0, mode='FAN_IN', uniform=False), data_format='channels_last')\\n    squeeze_excite = tf.expand_dims(tf.nn.sigmoid(squeeze_excite), 1)\\n    pattern = tf.stack([1, num_frames, 1, 1, 1])\\n    return tf.reshape(tf.tile(squeeze_excite, pattern), feature_shape) * inputs [Docstring] Squeeze and excite layer for videos.\\n\\nSqueeze-and-Excitation Networks\\narXiv: 1709.01507\\n\\nArgs:\\n  inputs: `Tensor` of size `[batch*time, height, width, channels]`. Only\\n    supports 'channels_last' as the data format.\\n  filters: `int` number of filters in the convolution.\\n  ratio: 'float' percent to squeeze\\n  num_frames: int number of frames\\n\\nReturns:\\n  A `Tensor` of the same data_format [EOS]\n",
            "\n",
            "[Function] def CRNN2D(X_shape, nb_classes):\\n    nb_layers = 4\\n    nb_filters = [64, 128, 128, 128]\\n    kernel_size = (3, 3)\\n    activation = 'elu'\\n    pool_size = [(2, 2), (4, 2), (4, 2), (4, 2), (4, 2)]\\n    input_shape = (X_shape[1], X_shape[2], X_shape[3])\\n    frequency_axis = 1\\n    time_axis = 2\\n    channel_axis = 3\\n    model = Sequential()\\n    model.add(BatchNormalization(axis=frequency_axis, input_shape=input_shape))\\n    model.add(Conv2D(nb_filters[0], kernel_size=kernel_size, padding='same', data_format='channels_last', input_shape=input_shape))\\n    model.add(Activation(activation))\\n    model.add(BatchNormalization(axis=channel_axis))\\n    model.add(MaxPooling2D(pool_size=pool_size[0], strides=pool_size[0]))\\n    model.add(Dropout(0.1))\\n    for layer in range(nb_layers - 1):\\n        model.add(Conv2D(nb_filters[layer + 1], kernel_size=kernel_size, padding='same'))\\n        model.add(Activation(activation))\\n        model.add(BatchNormalization(axis=channel_axis))\\n        model.add(MaxPooling2D(pool_size=pool_size[layer + 1], strides=pool_size[layer + 1]))\\n        model.add(Dropout(0.1))\\n    model.add(Permute((time_axis, frequency_axis, channel_axis)))\\n    resize_shape = model.output_shape[2] * model.output_shape[3]\\n    model.add(Reshape((model.output_shape[1], resize_shape)))\\n    model.add(GRU(32, return_sequences=True))\\n    model.add(GRU(32, return_sequences=False))\\n    model.add(Dropout(0.3))\\n    model.add(Dense(nb_classes))\\n    model.add(Activation('softmax'))\\n    return model [Docstring] Model used for evaluation in paper. Inspired by K. Choi model in:\\nhttps://github.com/keunwoochoi/music-auto_tagging-keras/blob/master/music_tagger_crnn.py [EOS]\n",
            "\n",
            "[Function] def _validate_label_map(label_map):\\n    for item in label_map.item:\\n        if item.id < 1:\\n            raise ValueError('Label map ids should be >= 1.') [Docstring] Checks if a label map is valid.\\n\\nArgs:\\n  label_map: StringIntLabelMap to validate.\\n\\nRaises:\\n  ValueError: if label map is invalid. [EOS]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def file_ready(filepath, min_size):\n",
        "    \"\"\"Check whether the file exists and exceeds a minimum size.\"\"\"\n",
        "    return os.path.isfile(filepath) and os.path.getsize(filepath) >= min_size\n",
        "\n",
        "\n",
        "timeout = 60\n",
        "min_size = 42\n",
        "start_time = time.time()\n",
        "\n",
        "while not (file_ready(test_file, min_size) and file_ready(train_file, min_size)):\n",
        "    elapsed_time = time.time() - start_time\n",
        "    if elapsed_time > timeout:\n",
        "        print(\"Timeout\")\n",
        "        break\n",
        "else:\n",
        "    print(\"TEST (File size\", os.path.getsize(test_file), \"bytes):\")\n",
        "    with open(test_file) as f:\n",
        "        for i, line in enumerate(f):\n",
        "            print(line)\n",
        "            if i == 2: break\n",
        "\n",
        "    print(\"TRAIN (File size\", os.path.getsize(train_file), \"bytes):\")\n",
        "    with open(train_file) as f:\n",
        "        for i, line in enumerate(f):\n",
        "            print(line)\n",
        "            if i == 2: break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "y0qwY65ncHOW"
      },
      "outputs": [],
      "source": [
        "def load_dataset_from_text_files(train_file_path, test_file_path):\n",
        "    \"\"\"\n",
        "    Load training and test datasets from text files into a DatasetDict.\n",
        "\n",
        "    Args:\n",
        "        train_file_path (str): Path to the training data text file.\n",
        "        test_file_path (str): Path to the test data text file.\n",
        "\n",
        "    Returns:\n",
        "        DatasetDict: A dictionary containing 'train' and 'test' datasets with text data.\n",
        "    \"\"\"\n",
        "\n",
        "    def get_lines_from_file(file_path):\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            lines = [line.strip() for line in file if line.strip()]\n",
        "        print(f'Total lines loaded from {file_path}: {len(lines)}')\n",
        "        return lines\n",
        "\n",
        "    def get_dataset_generator(file_path):\n",
        "        lines = get_lines_from_file(file_path)\n",
        "        for line in lines:\n",
        "            yield {\"text\": line}\n",
        "\n",
        "    dataset_train = Dataset.from_generator(\n",
        "        generator=lambda: get_dataset_generator(train_file_path),\n",
        "        features=Features({'text': Value('string')})\n",
        "    )\n",
        "    dataset_test = Dataset.from_generator(\n",
        "        generator=lambda: get_dataset_generator(test_file_path),\n",
        "        features=Features({'text': Value('string')})\n",
        "    )\n",
        "    return DatasetDict({\"train\": dataset_train, \"test\": dataset_test})\n",
        "\n",
        "\n",
        "datasets = load_dataset_from_text_files(train_file, test_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "b68264a60c984614aa4b6b6de2a31e7b",
            "d92bb162dcb74c149f54db3d035d2805",
            "62f0778cf823447da8e80cb56268701e",
            "7eb1158c75484a418bfcbe52764cb7e7",
            "d7b75f2d30cd49d7b3e5c982ea9de65c",
            "f29c0059b57c4084b18c4089b97873ad",
            "74f190367ca649b38c8ea3fbdd2ff93e",
            "95e63e62a84c482081243766b58f88d7",
            "77915f031ee8435c945bc06da6da0256",
            "7114f9d786b54139981caec48b3a0b38",
            "50ffacca6cdc4e9285c226b703ff988e"
          ]
        },
        "id": "syHu-xuHcHOX",
        "outputId": "5de95170-9dde-46bd-b35f-e577806fd281"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Notebook is running on Colab: Using 4-bit quantization.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b68264a60c984614aa4b6b6de2a31e7b"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Model selection and configuration\n",
        "model_to_finetune = \"meta-llama/CodeLlama-7b-Python-hf\"\n",
        "# model_to_finetune = \"tiiuae/falcon-rw-1b\"\n",
        "# model_to_finetune = \"tiiuae/falcon-7b\"\n",
        "\n",
        "if on_colab:\n",
        "    quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
        "    torch_dtype = None\n",
        "    device_map = \"auto\"\n",
        "    print(\"Notebook is running on Colab: Using 4-bit quantization.\")\n",
        "else:\n",
        "    quantization_config = None\n",
        "    torch_dtype = torch.bfloat16\n",
        "    device_map = \"cpu\"\n",
        "    print(\"Notebook is running locally: Using bfloat16 precision.\")\n",
        "\n",
        "# Load tokeniser and model with auth token\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_to_finetune,\n",
        "    token=huggingface_api_token,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_to_finetune,\n",
        "    token=huggingface_api_token,\n",
        "    device_map=device_map,\n",
        "    quantization_config=quantization_config,\n",
        "    torch_dtype=torch_dtype,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "# Deactivating cache & setting pretraining\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYzTncnpcHOX",
        "outputId": "76722e82-366e-4215-a4ca-79c9c6d62070"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train example 6904 : {'text': '[Function] @pytest.fixture(scope=\\'function\\')\\\\ndef iframed_app(page: Page, app_port: int) -> IframedPage:\\\\n    fake_iframe_server_origin = \\'http://localhost:1345\\'\\\\n    fake_iframe_server_route = f\\'{fake_iframe_server_origin}/iframed_app.html\\'\\\\n    app_url = f\\'http://localhost:{app_port}\\'\\\\n    app_csp_header = f\"default-src \\'none\\'; worker-src blob:; form-action \\'none\\'; connect-src ws://localhost:{app_port}/_stcore/stream http://localhost:{app_port}/_stcore/allowed-message-origins http://localhost:{app_port}/_stcore/upload_file/ https://some-prefix.com/somethingelse/_stcore/upload_file/ http://localhost:{app_port}/_stcore/host-config http://localhost:{app_port}/_stcore/health; script-src \\'unsafe-inline\\' \\'unsafe-eval\\' {app_url}/static/js/; style-src \\'unsafe-inline\\' {app_url}/static/css/; img-src data: {app_url}/favicon.png {app_url}/favicon.ico; font-src {app_url}/static/fonts/ {app_url}/static/media/; frame-ancestors {fake_iframe_server_origin};\"\\\\n\\\\n    def _open_app(iframe_element_attrs: IframedPageAttrs | None=None) -> FrameLocator:\\\\n        _iframe_element_attrs = iframe_element_attrs\\\\n        if _iframe_element_attrs is None:\\\\n            _iframe_element_attrs = IframedPageAttrs()\\\\n        query_params = \\'\\'\\\\n        if _iframe_element_attrs.src_query_params:\\\\n            query_params = \\'?\\' + parse.urlencode(_iframe_element_attrs.src_query_params)\\\\n        src = f\\'{app_url}/{query_params}\\'\\\\n        additional_html_head = _iframe_element_attrs.additional_html_head if _iframe_element_attrs.additional_html_head else \\'\\'\\\\n        _iframed_body = f\"\"\"\\\\n            <!DOCTYPE html>\\\\n            <html style=\"height: 100%;\">\\\\n                <head>\\\\n                    <meta charset=\"UTF-8\">\\\\n                    <title>Iframed Streamlit App</title>\\\\n                    {additional_html_head}\\\\n                </head>\\\\n                <body style=\"height: 100%;\">\\\\n                    <iframe\\\\n                        src={src}\\\\n                        id={(_iframe_element_attrs.element_id if _iframe_element_attrs.element_id else \\'\\')}\\\\n                        title=\"Iframed Streamlit App\"\\\\n                        allow=\"clipboard-write; microphone;\"\\\\n                        sandbox=\"allow-popups allow-same-origin allow-scripts allow-downloads\"\\\\n                        width=\"100%\"\\\\n                    >\\\\n                    </iframe>\\\\n                </body>\\\\n            </html>\\\\n            \"\"\" if _iframe_element_attrs.html_content is None else _iframe_element_attrs.html_content.replace(\\'$APP_URL\\', app_url)\\\\n\\\\n        def fulfill_iframe_request(route: Route) -> None:\\\\n            \"\"\"Return as response an iframe that loads the actual Streamlit app.\"\"\"\\\\n            browser = page.context.browser\\\\n            frame_src_blob = \\'\\'\\\\n            if browser is not None and (browser.browser_type.name == \\'webkit\\' or browser.browser_type.name == \\'firefox\\'):\\\\n                frame_src_blob = \\'blob:\\'\\\\n            route.fulfill(status=200, body=_iframed_body, headers={\\'Content-Type\\': \\'text/html\\', \\'Content-Security-Policy\\': f\\'frame-src {frame_src_blob} {app_url};\\'})\\\\n        page.route(fake_iframe_server_route, fulfill_iframe_request)\\\\n\\\\n        def fullfill_streamlit_app_request(route: Route) -> None:\\\\n            \"\"\"Get the actual Streamlit app and return it\\'s content.\"\"\"\\\\n            response = route.fetch()\\\\n            route.fulfill(body=response.body(), headers={**response.headers, \\'Content-Security-Policy\\': app_csp_header})\\\\n        page.route(src, fullfill_streamlit_app_request)\\\\n\\\\n        def _expect_streamlit_app_loaded_in_iframe_with_added_header(response: Response) -> bool:\\\\n            \"\"\"Ensure that the routing-interception worked and that Streamlit app is\\\\n            indeed loaded with the CSP header we expect\"\"\"\\\\n            return response.url == src and response.headers[\\'content-security-policy\\'] == app_csp_header\\\\n        with page.expect_event(\\'response\\', predicate=_expect_streamlit_app_loaded_in_iframe_with_added_header):\\\\n            page.goto(fake_iframe_server_route, wait_until=\\'domcontentloaded\\')\\\\n            frame_locator = page.frame_locator(\\'iframe\\')\\\\n            frame_locator.nth(0).get_by_test_id(\\'stAppViewContainer\\').wait_for(timeout=30000, state=\\'attached\\')\\\\n        return frame_locator\\\\n    return IframedPage(page, _open_app) [Docstring] Fixture that returns an IframedPage.\\\\n\\\\nThe page object can be used to configure additional routes, for example to override\\\\nthe host-config. The open_app function triggers the opening of the app in an iframe. [EOS]', 'input_ids': [1, 518, 6678, 29962, 732, 2272, 1688, 29889, 7241, 15546, 29898, 6078, 2433, 2220, 1495, 29905, 299, 1389, 565, 2572, 287, 29918, 932, 29898, 3488, 29901, 9305, 29892, 623, 29918, 637, 29901, 938, 29897, 1599, 960, 2572, 287, 5074, 3583, 29876, 1678, 25713, 29918, 22000, 29918, 2974, 29918, 12574, 353, 525, 1124, 597, 7640, 29901, 29896, 29941, 29946, 29945, 12764, 29876, 1678, 25713, 29918, 22000, 29918, 2974, 29918, 13134, 353, 285, 29915, 29912, 29888, 1296, 29918, 22000, 29918, 2974, 29918, 12574, 6822, 361, 2572, 287, 29918, 932, 29889, 1420, 12764, 29876, 1678, 623, 29918, 2271, 353, 285, 29915, 1124, 597, 7640, 26254, 932, 29918, 637, 10162, 29905, 29876, 1678, 623, 29918, 29883, 1028, 29918, 6672, 353, 285, 29908, 4381, 29899, 4351, 525, 9290, 2670, 15645, 29899, 4351, 23755, 29901, 29936, 883, 29899, 2467, 525, 9290, 2670, 4511, 29899, 4351, 16904, 597, 7640, 26254, 932, 29918, 637, 6822, 29918, 303, 3221, 29914, 5461, 1732, 597, 7640, 26254, 932, 29918, 637, 6822, 29918, 303, 3221, 29914, 24622, 29899, 4906, 29899, 12683, 1144, 1732, 597, 7640, 26254, 932, 29918, 637, 6822, 29918, 303, 3221, 29914, 9009, 29918, 1445, 29914, 2045, 597, 5372, 29899, 13506, 29889, 510, 29914, 14481, 2870, 19891, 303, 3221, 29914, 9009, 29918, 1445, 29914, 1732, 597, 7640, 26254, 932, 29918, 637, 6822, 29918, 303, 3221, 29914, 3069, 29899, 2917, 1732, 597, 7640, 26254, 932, 29918, 637, 6822, 29918, 303, 3221, 29914, 354, 4298, 29936, 2471, 29899, 4351, 525, 348, 11177, 29899, 14764, 29915, 525, 348, 11177, 29899, 14513, 29915, 426, 932, 29918, 2271, 6822, 7959, 29914, 1315, 29914, 29936, 3114, 29899, 4351, 525, 348, 11177, 29899, 14764, 29915, 426, 932, 29918, 2271, 6822, 7959, 29914, 4268, 29914, 29936, 10153, 29899, 4351, 848, 29901, 426, 932, 29918, 2271, 6822, 29888, 485, 4144, 29889, 2732, 426, 932, 29918, 2271, 6822, 29888, 485, 4144, 29889, 1417, 29936, 4079, 29899, 4351, 426, 932, 29918, 2271, 6822, 7959, 29914, 28586, 29914, 426, 932, 29918, 2271, 6822, 7959, 29914, 9799, 29914, 29936, 3515, 29899, 4564, 342, 943, 426, 29888, 1296, 29918, 22000, 29918, 2974, 29918, 12574, 3400, 26732, 29876, 29905, 29876, 1678, 822, 903, 3150, 29918, 932, 29898, 22000, 29918, 5029, 29918, 5552, 29879, 29901, 960, 2572, 287, 5074, 25098, 29879, 891, 6213, 29922, 8516, 29897, 1599, 12218, 3524, 1061, 3583, 29876, 4706, 903, 22000, 29918, 5029, 29918, 5552, 29879, 353, 22014, 29918, 5029, 29918, 5552, 29879, 29905, 29876, 4706, 565, 903, 22000, 29918, 5029, 29918, 5552, 29879, 338, 6213, 3583, 29876, 9651, 903, 22000, 29918, 5029, 29918, 5552, 29879, 353, 960, 2572, 287, 5074, 25098, 29879, 580, 29905, 29876, 4706, 2346, 29918, 7529, 353, 6629, 29905, 29876, 4706, 565, 903, 22000, 29918, 5029, 29918, 5552, 29879, 29889, 4351, 29918, 1972, 29918, 7529, 3583, 29876, 9651, 2346, 29918, 7529, 353, 525, 17901, 718, 6088, 29889, 2271, 12508, 7373, 22000, 29918, 5029, 29918, 5552, 29879, 29889, 4351, 29918, 1972, 29918, 7529, 2144, 29876, 4706, 4765, 353, 285, 29915, 29912, 932, 29918, 2271, 6822, 29912, 1972, 29918, 7529, 10162, 29905, 29876, 4706, 5684, 29918, 1420, 29918, 2813, 353, 903, 22000, 29918, 5029, 29918, 5552, 29879, 29889, 1202, 3245, 29918, 1420, 29918, 2813, 565, 903, 22000, 29918, 5029, 29918, 5552, 29879, 29889, 1202, 3245, 29918, 1420, 29918, 2813, 1683, 6629, 29905, 29876, 4706, 903, 361, 2572, 287, 29918, 2587, 353, 285, 15945, 26732, 29876, 9651, 18252, 21300, 3472, 14247, 29876, 9651, 529, 1420, 3114, 543, 3545, 29901, 29871, 29896, 29900, 29900, 8874, 1013, 29905, 29876, 18884, 529, 2813, 14247, 29876, 462, 1678, 529, 7299, 17425, 543, 10496, 29899, 29947, 1013, 29905, 29876, 462, 1678, 529, 3257, 29958, 29902, 1341, 2795, 13763, 19411, 2401, 829, 3257, 14247, 29876, 462, 1678, 426, 1202, 3245, 29918, 1420, 29918, 2813, 1012, 29876, 18884, 1533, 2813, 14247, 29876, 18884, 529, 2587, 3114, 543, 3545, 29901, 29871, 29896, 29900, 29900, 8874, 1013, 29905, 29876, 462, 1678, 529, 22000, 29905, 29876, 462, 4706, 4765, 3790, 4351, 1012, 29876, 462, 4706, 1178, 3790, 7373, 22000, 29918, 5029, 29918, 5552, 29879, 29889, 5029, 29918, 333, 565, 903, 22000, 29918, 5029, 29918, 5552, 29879, 29889, 5029, 29918, 333, 1683, 27255, 1012, 29876, 462, 4706, 3611, 543, 29902, 1341, 2795, 13763, 19411, 2401, 26732, 29876, 462, 4706, 2758, 543, 24049, 3377, 29899, 3539, 29936, 9200, 6710, 15458, 29905, 29876, 462, 4706, 11982, 1884, 543, 9536, 29899, 7323, 14340, 2758, 29899, 17642, 29899, 12574, 2758, 29899, 16713, 2758, 29899, 10382, 29879, 26732, 29876, 462, 4706, 2920, 543, 29896, 29900, 29900, 23577, 29905, 29876, 462, 1678, 1405, 29905, 29876, 462, 1678, 1533, 22000, 14247, 29876, 18884, 1533, 2587, 14247, 29876, 9651, 1533, 1420, 14247, 29876, 9651, 9995, 565, 903, 22000, 29918, 5029, 29918, 5552, 29879, 29889, 1420, 29918, 3051, 338, 6213, 1683, 903, 22000, 29918, 5029, 29918, 5552, 29879, 29889, 1420, 29918, 3051, 29889, 6506, 877, 29938, 20576, 29918, 4219, 742, 623, 29918, 2271, 2144, 29876, 29905, 29876, 4706, 822, 6095, 5589, 29918, 22000, 29918, 3827, 29898, 13134, 29901, 12034, 29897, 1599, 6213, 3583, 29876, 9651, 9995, 11609, 408, 2933, 385, 22014, 393, 15376, 278, 3935, 13763, 19411, 623, 1213, 15945, 29905, 29876, 9651, 4714, 353, 1813, 29889, 4703, 29889, 15965, 29905, 29876, 9651, 3515, 29918, 4351, 29918, 10054, 353, 6629, 29905, 29876, 9651, 565, 4714, 338, 451, 6213, 322, 313, 15965, 29889, 15965, 29918, 1853, 29889, 978, 1275, 525, 14085, 29915, 470, 4714, 29889, 15965, 29918, 1853, 29889, 978, 1275, 525, 8696, 8944, 29374, 29905, 29876, 18884, 3515, 29918, 4351, 29918, 10054, 353, 525, 10054, 11283, 29905, 29876, 9651, 5782, 29889, 1319, 5589, 29898, 4882, 29922, 29906, 29900, 29900, 29892, 3573, 29922, 29918, 361, 2572, 287, 29918, 2587, 29892, 9066, 3790, 29915, 3916, 29899, 1542, 2396, 525, 726, 29914, 1420, 742, 525, 3916, 29899, 13228, 29899, 15644, 2396, 285, 29915, 2557, 29899, 4351, 426, 2557, 29918, 4351, 29918, 10054, 29913, 426, 932, 29918, 2271, 3400, 29915, 11606, 29876, 4706, 1813, 29889, 13134, 29898, 29888, 1296, 29918, 22000, 29918, 2974, 29918, 13134, 29892, 6095, 5589, 29918, 22000, 29918, 3827, 2144, 29876, 29905, 29876, 4706, 822, 2989, 5589, 29918, 5461, 19411, 29918, 932, 29918, 3827, 29898, 13134, 29901, 12034, 29897, 1599, 6213, 3583, 29876, 9651, 9995, 2577, 278, 3935, 13763, 19411, 623, 322, 736, 372, 29915, 29879, 2793, 1213, 15945, 29905, 29876, 9651, 2933, 353, 5782, 29889, 9155, 580, 29905, 29876, 9651, 5782, 29889, 1319, 5589, 29898, 2587, 29922, 5327, 29889, 2587, 3285, 9066, 3790, 1068, 5327, 29889, 13662, 29892, 525, 3916, 29899, 13228, 29899, 15644, 2396, 623, 29918, 29883, 1028, 29918, 6672, 11606, 29876, 4706, 1813, 29889, 13134, 29898, 4351, 29892, 2989, 5589, 29918, 5461, 19411, 29918, 932, 29918, 3827, 2144, 29876, 29905, 29876, 4706, 822, 903, 17854, 29918, 5461, 19411, 29918, 932, 29918, 15638, 29918, 262, 29918, 22000, 29918, 2541, 29918, 23959, 29918, 6672, 29898, 5327, 29901, 13291, 29897, 1599, 6120, 3583, 29876, 9651, 9995, 29923, 1983, 545, 393, 278, 21398, 29899, 1639, 1441, 3796, 322, 393, 13763, 19411, 623, 338, 29905, 29876, 9651, 6200, 7500, 411, 278, 315, 5550, 4839, 591, 2149, 15945, 26732, 29876, 9651, 736, 2933, 29889, 2271, 1275, 4765, 322, 2933, 29889, 13662, 1839, 3051, 29899, 8926, 29899, 22197, 2033, 1275, 623, 29918, 29883, 1028, 29918, 6672, 29905, 29876, 4706, 411, 1813, 29889, 17854, 29918, 3696, 877, 5327, 742, 24384, 29922, 29918, 17854, 29918, 5461, 19411, 29918, 932, 29918, 15638, 29918, 262, 29918, 22000, 29918, 2541, 29918, 23959, 29918, 6672, 1125, 29905, 29876, 9651, 1813, 29889, 27102, 29898, 29888, 1296, 29918, 22000, 29918, 2974, 29918, 13134, 29892, 4480, 29918, 29305, 2433, 3129, 3051, 15638, 1495, 29905, 29876, 9651, 3515, 29918, 2029, 1061, 353, 1813, 29889, 2557, 29918, 2029, 1061, 877, 22000, 1495, 29905, 29876, 9651, 3515, 29918, 2029, 1061, 29889, 20800, 29898, 29900, 467, 657, 29918, 1609, 29918, 1688, 29918, 333, 877, 303, 2052, 1043, 7895, 2824, 10685, 29918, 1454, 29898, 15619, 29922, 29941, 29900, 29900, 29900, 29900, 29892, 2106, 2433, 1131, 3791, 1495, 29905, 29876, 4706, 736, 3515, 29918, 2029, 1061, 29905, 29876, 1678, 736, 960, 2572, 287, 5074, 29898, 3488, 29892, 903, 3150, 29918, 932, 29897, 518, 14526, 1807, 29962, 383, 29875, 15546, 393, 3639, 385, 960, 2572, 287, 5074, 7790, 29876, 29905, 29876, 1576, 1813, 1203, 508, 367, 1304, 304, 10822, 5684, 12049, 29892, 363, 1342, 304, 5712, 29905, 593, 354, 3495, 29899, 2917, 29889, 450, 1722, 29918, 932, 740, 23660, 278, 8718, 310, 278, 623, 297, 385, 22014, 29889, 518, 29923, 3267, 29962], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
            "Train example 9238 : {'text': '[Function] def process_batch(self, detections, labels):\\\\n    detections = detections[detections[:, 4] > self.conf]\\\\n    gt_classes = labels[:, 0].int()\\\\n    detection_classes = detections[:, 5].int()\\\\n    iou = general.box_iou(labels[:, 1:], detections[:, :4])\\\\n    x = torch.where(iou > self.iou_thres)\\\\n    if x[0].shape[0]:\\\\n        matches = torch.cat((torch.stack(x, 1), iou[x[0], x[1]][:, None]), 1).cpu().numpy()\\\\n        if x[0].shape[0] > 1:\\\\n            matches = matches[matches[:, 2].argsort()[::-1]]\\\\n            matches = matches[np.unique(matches[:, 1], return_index=True)[1]]\\\\n            matches = matches[matches[:, 2].argsort()[::-1]]\\\\n            matches = matches[np.unique(matches[:, 0], return_index=True)[1]]\\\\n    else:\\\\n        matches = np.zeros((0, 3))\\\\n    n = matches.shape[0] > 0\\\\n    (m0, m1, _) = matches.transpose().astype(np.int16)\\\\n    for (i, gc) in enumerate(gt_classes):\\\\n        j = m0 == i\\\\n        if n and sum(j) == 1:\\\\n            self.matrix[gc, detection_classes[m1[j]]] += 1\\\\n        else:\\\\n            self.matrix[gc, self.nc] += 1\\\\n    if n:\\\\n        for (i, dc) in enumerate(detection_classes):\\\\n            if not any(m1 == i):\\\\n                self.matrix[self.nc, dc] += 1 [Docstring] Return intersection-over-union (Jaccard index) of boxes.\\\\nBoth sets of boxes are expected to be in (x1, y1, x2, y2) format.\\\\nArguments:\\\\n    detections (Array[N, 6]), x1, y1, x2, y2, conf, class\\\\n    labels (Array[M, 5]), class, x1, y1, x2, y2\\\\nReturns:\\\\n    None, updates confusion matrix accordingly [EOS]', 'input_ids': [1, 518, 6678, 29962, 822, 1889, 29918, 16175, 29898, 1311, 29892, 1439, 29872, 1953, 29892, 11073, 1125, 29905, 29876, 1678, 1439, 29872, 1953, 353, 1439, 29872, 1953, 29961, 29881, 2650, 1953, 7503, 29892, 29871, 29946, 29962, 1405, 1583, 29889, 5527, 10725, 29876, 1678, 330, 29873, 29918, 13203, 353, 11073, 7503, 29892, 29871, 29900, 1822, 524, 580, 29905, 29876, 1678, 15326, 29918, 13203, 353, 1439, 29872, 1953, 7503, 29892, 29871, 29945, 1822, 524, 580, 29905, 29876, 1678, 474, 283, 353, 2498, 29889, 1884, 29918, 29875, 283, 29898, 21134, 7503, 29892, 29871, 29896, 29901, 1402, 1439, 29872, 1953, 7503, 29892, 584, 29946, 29962, 2144, 29876, 1678, 921, 353, 4842, 305, 29889, 3062, 29898, 29875, 283, 1405, 1583, 29889, 29875, 283, 29918, 386, 690, 2144, 29876, 1678, 565, 921, 29961, 29900, 1822, 12181, 29961, 29900, 29962, 3583, 29876, 4706, 7087, 353, 4842, 305, 29889, 4117, 3552, 7345, 305, 29889, 1429, 29898, 29916, 29892, 29871, 29896, 511, 474, 283, 29961, 29916, 29961, 29900, 1402, 921, 29961, 29896, 29962, 3816, 29901, 29892, 6213, 11724, 29871, 29896, 467, 21970, 2141, 23749, 580, 29905, 29876, 4706, 565, 921, 29961, 29900, 1822, 12181, 29961, 29900, 29962, 1405, 29871, 29896, 3583, 29876, 9651, 7087, 353, 7087, 29961, 20317, 7503, 29892, 29871, 29906, 1822, 5085, 441, 580, 29961, 1057, 29899, 29896, 5262, 29905, 29876, 9651, 7087, 353, 7087, 29961, 9302, 29889, 13092, 29898, 20317, 7503, 29892, 29871, 29896, 1402, 736, 29918, 2248, 29922, 5574, 9601, 29896, 5262, 29905, 29876, 9651, 7087, 353, 7087, 29961, 20317, 7503, 29892, 29871, 29906, 1822, 5085, 441, 580, 29961, 1057, 29899, 29896, 5262, 29905, 29876, 9651, 7087, 353, 7087, 29961, 9302, 29889, 13092, 29898, 20317, 7503, 29892, 29871, 29900, 1402, 736, 29918, 2248, 29922, 5574, 9601, 29896, 5262, 29905, 29876, 1678, 1683, 3583, 29876, 4706, 7087, 353, 7442, 29889, 3298, 359, 3552, 29900, 29892, 29871, 29941, 28986, 29876, 1678, 302, 353, 7087, 29889, 12181, 29961, 29900, 29962, 1405, 29871, 29900, 29905, 29876, 1678, 313, 29885, 29900, 29892, 286, 29896, 29892, 24459, 353, 7087, 29889, 3286, 4220, 2141, 579, 668, 29898, 9302, 29889, 524, 29896, 29953, 2144, 29876, 1678, 363, 313, 29875, 29892, 330, 29883, 29897, 297, 26985, 29898, 4141, 29918, 13203, 1125, 29905, 29876, 4706, 432, 353, 286, 29900, 1275, 474, 29905, 29876, 4706, 565, 302, 322, 2533, 29898, 29926, 29897, 1275, 29871, 29896, 3583, 29876, 9651, 1583, 29889, 5344, 29961, 27354, 29892, 15326, 29918, 13203, 29961, 29885, 29896, 29961, 29926, 5262, 29962, 4619, 29871, 29896, 29905, 29876, 4706, 1683, 3583, 29876, 9651, 1583, 29889, 5344, 29961, 27354, 29892, 1583, 29889, 17608, 29962, 4619, 29871, 29896, 29905, 29876, 1678, 565, 302, 3583, 29876, 4706, 363, 313, 29875, 29892, 270, 29883, 29897, 297, 26985, 29898, 29881, 2650, 428, 29918, 13203, 1125, 29905, 29876, 9651, 565, 451, 738, 29898, 29885, 29896, 1275, 474, 1125, 29905, 29876, 18884, 1583, 29889, 5344, 29961, 1311, 29889, 17608, 29892, 270, 29883, 29962, 4619, 29871, 29896, 518, 14526, 1807, 29962, 7106, 17686, 29899, 957, 29899, 13094, 313, 29967, 5753, 538, 2380, 29897, 310, 16273, 7790, 29876, 29933, 720, 6166, 310, 16273, 526, 3806, 304, 367, 297, 313, 29916, 29896, 29892, 343, 29896, 29892, 921, 29906, 29892, 343, 29906, 29897, 3402, 7790, 29876, 26915, 3583, 29876, 1678, 1439, 29872, 1953, 313, 2588, 29961, 29940, 29892, 29871, 29953, 11724, 921, 29896, 29892, 343, 29896, 29892, 921, 29906, 29892, 343, 29906, 29892, 1970, 29892, 770, 29905, 29876, 1678, 11073, 313, 2588, 29961, 29924, 29892, 29871, 29945, 11724, 770, 29892, 921, 29896, 29892, 343, 29896, 29892, 921, 29906, 29892, 343, 29906, 29905, 29876, 11609, 29879, 3583, 29876, 1678, 6213, 29892, 11217, 14679, 4636, 16205, 518, 29923, 3267, 29962], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
            "Train example 7382 : {'text': \"[Function] def image_normalize(input_image, warning=True, debug=True):\\\\n    (np_image, isnan) = safe_image_like(input_image, warning=warning, debug=debug)\\\\n    if isuintnparray(np_image):\\\\n        np_image = np_image.astype('float32') / 255.0\\\\n    else:\\\\n        assert isfloatnparray(np_image), 'the input image-like array should be either an uint8 or float32 array'\\\\n    min_val = np.min(np_image)\\\\n    max_val = np.max(np_image)\\\\n    if isnan:\\\\n        np_image.fill(0)\\\\n    elif min_val == max_val:\\\\n        if warning:\\\\n            print('the input image has the same value over all the pixels')\\\\n        np_image.fill(0)\\\\n    else:\\\\n        np_image -= min_val\\\\n        np_image = (np_image / (max_val - min_val) * 255.0).astype('uint8')\\\\n        if debug:\\\\n            assert np.min(np_image) == 0 and np.max(np_image) == 255, 'the value range is not right [%f, %f]' % (np.min(np_image), np.max(np_image))\\\\n    return np_image.astype('uint8') [Docstring] normalize an image to an uint8 with range of [0, 255]\\\\nnote that: the input might not be an image because the value range might be arbitrary\\\\n\\\\nparameters:\\\\n        input_image:            pil image or image-like array, color or gray, float or uint\\\\n\\\\noutputs:\\\\n        np_image:                       numpy uint8 image, normalized to [0, 255] [EOS]\", 'input_ids': [1, 518, 6678, 29962, 822, 1967, 29918, 8945, 675, 29898, 2080, 29918, 3027, 29892, 9177, 29922, 5574, 29892, 4744, 29922, 5574, 1125, 29905, 29876, 1678, 313, 9302, 29918, 3027, 29892, 3508, 273, 29897, 353, 9109, 29918, 3027, 29918, 4561, 29898, 2080, 29918, 3027, 29892, 9177, 29922, 27392, 29892, 4744, 29922, 8382, 2144, 29876, 1678, 565, 338, 13470, 29876, 862, 764, 29898, 9302, 29918, 3027, 1125, 29905, 29876, 4706, 7442, 29918, 3027, 353, 7442, 29918, 3027, 29889, 579, 668, 877, 7411, 29941, 29906, 1495, 847, 29871, 29906, 29945, 29945, 29889, 29900, 29905, 29876, 1678, 1683, 3583, 29876, 4706, 4974, 338, 7411, 29876, 862, 764, 29898, 9302, 29918, 3027, 511, 525, 1552, 1881, 1967, 29899, 4561, 1409, 881, 367, 2845, 385, 13122, 29947, 470, 5785, 29941, 29906, 1409, 12764, 29876, 1678, 1375, 29918, 791, 353, 7442, 29889, 1195, 29898, 9302, 29918, 3027, 2144, 29876, 1678, 4236, 29918, 791, 353, 7442, 29889, 3317, 29898, 9302, 29918, 3027, 2144, 29876, 1678, 565, 3508, 273, 3583, 29876, 4706, 7442, 29918, 3027, 29889, 5589, 29898, 29900, 2144, 29876, 1678, 25342, 1375, 29918, 791, 1275, 4236, 29918, 791, 3583, 29876, 4706, 565, 9177, 3583, 29876, 9651, 1596, 877, 1552, 1881, 1967, 756, 278, 1021, 995, 975, 599, 278, 17036, 1495, 29905, 29876, 4706, 7442, 29918, 3027, 29889, 5589, 29898, 29900, 2144, 29876, 1678, 1683, 3583, 29876, 4706, 7442, 29918, 3027, 22361, 1375, 29918, 791, 29905, 29876, 4706, 7442, 29918, 3027, 353, 313, 9302, 29918, 3027, 847, 313, 3317, 29918, 791, 448, 1375, 29918, 791, 29897, 334, 29871, 29906, 29945, 29945, 29889, 29900, 467, 579, 668, 877, 13470, 29947, 1495, 29905, 29876, 4706, 565, 4744, 3583, 29876, 9651, 4974, 7442, 29889, 1195, 29898, 9302, 29918, 3027, 29897, 1275, 29871, 29900, 322, 7442, 29889, 3317, 29898, 9302, 29918, 3027, 29897, 1275, 29871, 29906, 29945, 29945, 29892, 525, 1552, 995, 3464, 338, 451, 1492, 518, 29995, 29888, 29892, 1273, 29888, 29962, 29915, 1273, 313, 9302, 29889, 1195, 29898, 9302, 29918, 3027, 511, 7442, 29889, 3317, 29898, 9302, 29918, 3027, 28986, 29876, 1678, 736, 7442, 29918, 3027, 29889, 579, 668, 877, 13470, 29947, 1495, 518, 14526, 1807, 29962, 4226, 675, 385, 1967, 304, 385, 13122, 29947, 411, 3464, 310, 518, 29900, 29892, 29871, 29906, 29945, 29945, 10725, 29876, 6812, 393, 29901, 278, 1881, 1795, 451, 367, 385, 1967, 1363, 278, 995, 3464, 1795, 367, 11472, 29905, 29876, 29905, 29876, 16744, 3583, 29876, 4706, 1881, 29918, 3027, 29901, 9651, 8230, 1967, 470, 1967, 29899, 4561, 1409, 29892, 2927, 470, 16749, 29892, 5785, 470, 13122, 29905, 29876, 29905, 29876, 4905, 29879, 3583, 29876, 4706, 7442, 29918, 3027, 29901, 462, 539, 12655, 13122, 29947, 1967, 29892, 4226, 1891, 304, 518, 29900, 29892, 29871, 29906, 29945, 29945, 29962, 518, 29923, 3267, 29962], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
            "Train example 8968 : {'text': '[Function] def rothesstri(A, b):\\\\n    n = shape(A)[0]\\\\n    A = hstack([A, b])\\\\n    for k in range(n - 1):\\\\n        r = linalg.norm([A[k, k], A[k + 1, k]])\\\\n        if r > 0:\\\\n            c = A[k, k] / r\\\\n            s = A[k + 1, k] / r\\\\n            A[[k, k + 1], k + 1:n + 1] = [[c, s], [-s, c]] * A[[k, k + 1], k + 1:n + 1]\\\\n        A[k, k] = r\\\\n        A[k + 1, k] = 0\\\\n    z = A[:, n].copy()\\\\n    rbacksolve(A[:, :n], z, n)\\\\n    return z [Docstring] Solve the system Ax=b where A is in upper Hessenberg form using \\\\nGivens rotations, using algorithm 4.36\\\\n\\\\nA: A matrix in upper Hessenberg form.\\\\nb: The right hand side. [EOS]', 'input_ids': [1, 518, 6678, 29962, 822, 696, 26041, 303, 374, 29898, 29909, 29892, 289, 1125, 29905, 29876, 1678, 302, 353, 8267, 29898, 29909, 9601, 29900, 10725, 29876, 1678, 319, 353, 298, 1429, 4197, 29909, 29892, 289, 29962, 2144, 29876, 1678, 363, 413, 297, 3464, 29898, 29876, 448, 29871, 29896, 1125, 29905, 29876, 4706, 364, 353, 301, 979, 29887, 29889, 12324, 4197, 29909, 29961, 29895, 29892, 413, 1402, 319, 29961, 29895, 718, 29871, 29896, 29892, 413, 5262, 2144, 29876, 4706, 565, 364, 1405, 29871, 29900, 3583, 29876, 9651, 274, 353, 319, 29961, 29895, 29892, 413, 29962, 847, 364, 29905, 29876, 9651, 269, 353, 319, 29961, 29895, 718, 29871, 29896, 29892, 413, 29962, 847, 364, 29905, 29876, 9651, 319, 8999, 29895, 29892, 413, 718, 29871, 29896, 1402, 413, 718, 29871, 29896, 29901, 29876, 718, 29871, 29896, 29962, 353, 5519, 29883, 29892, 269, 1402, 21069, 29879, 29892, 274, 5262, 334, 319, 8999, 29895, 29892, 413, 718, 29871, 29896, 1402, 413, 718, 29871, 29896, 29901, 29876, 718, 29871, 29896, 10725, 29876, 4706, 319, 29961, 29895, 29892, 413, 29962, 353, 364, 29905, 29876, 4706, 319, 29961, 29895, 718, 29871, 29896, 29892, 413, 29962, 353, 29871, 29900, 29905, 29876, 1678, 503, 353, 319, 7503, 29892, 302, 1822, 8552, 580, 29905, 29876, 1678, 364, 1627, 2929, 345, 29898, 29909, 7503, 29892, 584, 29876, 1402, 503, 29892, 302, 2144, 29876, 1678, 736, 503, 518, 14526, 1807, 29962, 4956, 345, 278, 1788, 22523, 29922, 29890, 988, 319, 338, 297, 7568, 26329, 2552, 883, 773, 320, 29876, 29954, 440, 575, 5731, 800, 29892, 773, 5687, 29871, 29946, 29889, 29941, 29953, 29905, 29876, 29905, 29876, 29909, 29901, 319, 4636, 297, 7568, 26329, 2552, 883, 7790, 9877, 29901, 450, 1492, 1361, 2625, 29889, 518, 29923, 3267, 29962], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
            "Train example 6445 : {'text': \"[Function] def _stacked_simclr_randaugment(image, warp_prob=0.5, blur_prob=0.5, strength=0.5, side_length=IMAGE_SIZE, use_pytorch_color_jitter=False):\\\\n    with tf.name_scope('stacked_simclr_randaugment'):\\\\n        image = _random_apply(functools.partial(_jitter_colors, strength=strength, use_pytorch_impl=use_pytorch_color_jitter), p=0.8, x=image)\\\\n        image = augment.RandAugment().distort(image)\\\\n        image = _random_apply(_rgb_to_gray, p=0.2, x=image)\\\\n        if warp_prob > 0.0:\\\\n            image = _random_apply(functools.partial(_warp, side_length=side_length), p=warp_prob, x=image)\\\\n        if blur_prob > 0.0:\\\\n            image = _convert_image_dtype(image, tf.float32)\\\\n            image = _random_apply(functools.partial(_gaussian_blur, side_length=side_length), p=blur_prob, x=image)\\\\n        return image [Docstring] A combination the data augmentation sequences from SimCLR and RandAugment.\\\\n\\\\nCitations:\\\\n  SimCLR: https://arxiv.org/abs/2002.05709\\\\n  RandAugment: https://arxiv.org/abs/1909.13719\\\\n\\\\nArgs:\\\\n  image: An image Tensor of shape [height, width, 3] and dtype tf.uint8.\\\\n  warp_prob: A Python float between 0 and 1. The probability of applying a\\\\n    warp transformation.\\\\n  blur_prob: A Python float between 0 and 1. The probability of applying a\\\\n    blur transformation.\\\\n  strength: strength: A Python float in range [0,1] controlling the maximum\\\\n    strength of the augmentations.\\\\n  side_length: A Python integer. The length, in pixels, of the width and\\\\n    height of `image`.\\\\n  use_pytorch_color_jitter: A Python bool. Whether to use a color jittering\\\\n    algorithm that aims to replicate `torchvision.transforms.ColorJitter`\\\\n    rather than the standard TensorFlow color jittering.\\\\n\\\\nReturns:\\\\n  An image with the same shape and dtype as the input image. [EOS]\", 'input_ids': [1, 518, 6678, 29962, 822, 903, 1429, 287, 29918, 3601, 695, 29878, 29918, 9502, 2987, 358, 29898, 3027, 29892, 1370, 29886, 29918, 22795, 29922, 29900, 29889, 29945, 29892, 1999, 332, 29918, 22795, 29922, 29900, 29889, 29945, 29892, 9324, 29922, 29900, 29889, 29945, 29892, 2625, 29918, 2848, 29922, 2382, 29918, 14226, 29892, 671, 29918, 2272, 7345, 305, 29918, 2780, 29918, 29926, 5171, 29922, 8824, 1125, 29905, 29876, 1678, 411, 15886, 29889, 978, 29918, 6078, 877, 1429, 287, 29918, 3601, 695, 29878, 29918, 9502, 2987, 358, 29374, 29905, 29876, 4706, 1967, 353, 903, 8172, 29918, 7302, 29898, 7692, 312, 8789, 29889, 3846, 7373, 29926, 5171, 29918, 27703, 29892, 9324, 29922, 710, 1477, 29892, 671, 29918, 2272, 7345, 305, 29918, 13699, 29922, 1509, 29918, 2272, 7345, 305, 29918, 2780, 29918, 29926, 5171, 511, 282, 29922, 29900, 29889, 29947, 29892, 921, 29922, 3027, 2144, 29876, 4706, 1967, 353, 18765, 29889, 29934, 392, 29909, 688, 358, 2141, 5721, 441, 29898, 3027, 2144, 29876, 4706, 1967, 353, 903, 8172, 29918, 7302, 7373, 23973, 29918, 517, 29918, 21012, 29892, 282, 29922, 29900, 29889, 29906, 29892, 921, 29922, 3027, 2144, 29876, 4706, 565, 1370, 29886, 29918, 22795, 1405, 29871, 29900, 29889, 29900, 3583, 29876, 9651, 1967, 353, 903, 8172, 29918, 7302, 29898, 7692, 312, 8789, 29889, 3846, 7373, 4495, 29886, 29892, 2625, 29918, 2848, 29922, 2975, 29918, 2848, 511, 282, 29922, 4495, 29886, 29918, 22795, 29892, 921, 29922, 3027, 2144, 29876, 4706, 565, 1999, 332, 29918, 22795, 1405, 29871, 29900, 29889, 29900, 3583, 29876, 9651, 1967, 353, 903, 13441, 29918, 3027, 29918, 29881, 1853, 29898, 3027, 29892, 15886, 29889, 7411, 29941, 29906, 2144, 29876, 9651, 1967, 353, 903, 8172, 29918, 7302, 29898, 7692, 312, 8789, 29889, 3846, 7373, 29887, 17019, 29918, 2204, 332, 29892, 2625, 29918, 2848, 29922, 2975, 29918, 2848, 511, 282, 29922, 2204, 332, 29918, 22795, 29892, 921, 29922, 3027, 2144, 29876, 4706, 736, 1967, 518, 14526, 1807, 29962, 319, 10296, 278, 848, 18765, 362, 15602, 515, 3439, 6154, 29934, 322, 17468, 29909, 688, 358, 7790, 29876, 29905, 29876, 29907, 24182, 3583, 29876, 29871, 3439, 6154, 29934, 29901, 2045, 597, 279, 26560, 29889, 990, 29914, 6897, 29914, 29906, 29900, 29900, 29906, 29889, 29900, 29945, 29955, 29900, 29929, 29905, 29876, 29871, 17468, 29909, 688, 358, 29901, 2045, 597, 279, 26560, 29889, 990, 29914, 6897, 29914, 29896, 29929, 29900, 29929, 29889, 29896, 29941, 29955, 29896, 29929, 29905, 29876, 29905, 29876, 7883, 3583, 29876, 29871, 1967, 29901, 530, 1967, 323, 6073, 310, 8267, 518, 3545, 29892, 2920, 29892, 29871, 29941, 29962, 322, 26688, 15886, 29889, 13470, 29947, 7790, 29876, 29871, 1370, 29886, 29918, 22795, 29901, 319, 5132, 5785, 1546, 29871, 29900, 322, 29871, 29896, 29889, 450, 6976, 310, 15399, 263, 29905, 29876, 1678, 1370, 29886, 13852, 7790, 29876, 29871, 1999, 332, 29918, 22795, 29901, 319, 5132, 5785, 1546, 29871, 29900, 322, 29871, 29896, 29889, 450, 6976, 310, 15399, 263, 29905, 29876, 1678, 1999, 332, 13852, 7790, 29876, 29871, 9324, 29901, 9324, 29901, 319, 5132, 5785, 297, 3464, 518, 29900, 29892, 29896, 29962, 640, 22155, 278, 7472, 29905, 29876, 1678, 9324, 310, 278, 18765, 800, 7790, 29876, 29871, 2625, 29918, 2848, 29901, 319, 5132, 6043, 29889, 450, 3309, 29892, 297, 17036, 29892, 310, 278, 2920, 322, 29905, 29876, 1678, 3171, 310, 421, 3027, 1412, 29905, 29876, 29871, 671, 29918, 2272, 7345, 305, 29918, 2780, 29918, 29926, 5171, 29901, 319, 5132, 6120, 29889, 26460, 304, 671, 263, 2927, 432, 5171, 292, 29905, 29876, 1678, 5687, 393, 263, 9893, 304, 1634, 5926, 421, 7345, 305, 4924, 29889, 9067, 29879, 29889, 3306, 29967, 5171, 29952, 29905, 29876, 1678, 3265, 1135, 278, 3918, 323, 6073, 17907, 2927, 432, 5171, 292, 7790, 29876, 29905, 29876, 11609, 29879, 3583, 29876, 29871, 530, 1967, 411, 278, 1021, 8267, 322, 26688, 408, 278, 1881, 1967, 29889, 518, 29923, 3267, 29962], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ]
        }
      ],
      "source": [
        "# Tokenisation of the data sets for training\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True)\n",
        "\n",
        "\n",
        "tokenized_datasets = datasets.map(tokenize_function, batched=True)\n",
        "\n",
        "# Example output of some training examples\n",
        "for i in range(5):\n",
        "    index = random.randint(0, len(tokenized_datasets[\"train\"]) - 1)\n",
        "    print(\"Train example\", index, \":\", tokenized_datasets[\"train\"][index])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jToDy5W5cHOY",
        "outputId": "a0d9323e-2fc3-4d6a-f18f-849716c8ff67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/nn/modules.py:451: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt 1 - Response:\n",
            " Write a docstring for the following Python code:\n",
            " [Function]\n",
            "def add_numbers(a, b): return a + b \n",
            " [Docstring]\n",
            "'''This function adds two numbers.''' \n",
            " [Testing]\n",
            ">>> add_numbers(1, 2) \n",
            "3 \n",
            "\n",
            "a = 10\n",
            "b = 20\n",
            "\n",
            "# Addition \n",
            "c = a + b \n",
            "\n",
            "# Subtraction \n",
            "c = a - b \n",
            "\n",
            "# Multiplication \n",
            "c = a * b \n",
            "\n",
            "# Division \n",
            "c = a / b \n",
            "\n",
            "# Mod\n",
            "Prompt 2 - Response:\n",
            " Write a docstring for the following Python code:\n",
            " [Function]\n",
            "def subtract_numbers(a, b): return a - b \n",
            " [Docstring]\n",
            "def subtract_numbers(a, b):\n",
            "    \"\"\"Returns the difference of two numbers.\n",
            "\n",
            "    Parameters\n",
            "    ----------\n",
            "    a : int, float\n",
            "        A number.\n",
            "    b : int, float\n",
            "        A number.\n",
            "\n",
            "    Returns\n",
            "    -------\n",
            "    int, float\n",
            "        The difference of a and b.\n",
            "\n",
            "    Examples\n",
            "    --------\n",
            "    >>> subtract_numbers(2, 4)\n",
            "    -\n"
          ]
        }
      ],
      "source": [
        "def test_model_response_pipeline(model, tokenizer, prompts, max_new_tokens=50):\n",
        "    \"\"\"\n",
        "    Tests the model's response to a list of prompts using Hugging Face's pipeline.\n",
        "\n",
        "    Args:\n",
        "        model (PreTrainedModel): The loaded model.\n",
        "        tokenizer (PreTrainedTokenizer): The tokenizer associated with the model.\n",
        "        prompts (list): A list of input prompts as strings.\n",
        "        max_new_tokens (int, optional): Maximum number of tokens to generate. Defaults to 50.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of the model's responses to the prompts.\n",
        "    \"\"\"\n",
        "    text_generator = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "    )\n",
        "\n",
        "    responses = [\n",
        "        text_generator(prompt, max_new_tokens=max_new_tokens, do_sample=True, top_k=10, temperature=0.7)[0][\n",
        "            \"generated_text\"]\n",
        "        for prompt in prompts\n",
        "    ]\n",
        "    return responses\n",
        "\n",
        "\n",
        "prompts = [\n",
        "    \"Write a docstring for the following Python code:\\n [Function]\\ndef add_numbers(a, b): return a + b \\n [Docstring]\\n\",\n",
        "    \"Write a docstring for the following Python code:\\n [Function]\\ndef subtract_numbers(a, b): return a - b \\n [Docstring]\\n\",\n",
        "]\n",
        "\n",
        "responses = test_model_response_pipeline(model, tokenizer, prompts, max_new_tokens=100)\n",
        "for i, response in enumerate(responses):\n",
        "    print(f\"Prompt {i + 1} - Response:\\n {response}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deZuemPOcHOY",
        "outputId": "51cf7126-11f2-4c67-f130-d73e7e02636d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "model\n",
            "model.embed_tokens\n",
            "model.layers\n",
            "model.layers.0\n",
            "model.layers.0.self_attn\n",
            "model.layers.0.self_attn.q_proj\n",
            "model.layers.0.self_attn.k_proj\n",
            "model.layers.0.self_attn.v_proj\n",
            "model.layers.0.self_attn.o_proj\n",
            "model.layers.0.mlp\n",
            "model.layers.0.mlp.gate_proj\n",
            "model.layers.0.mlp.up_proj\n",
            "model.layers.0.mlp.down_proj\n",
            "model.layers.0.mlp.act_fn\n",
            "model.layers.0.input_layernorm\n",
            "model.layers.0.post_attention_layernorm\n",
            "model.layers.1\n",
            "model.layers.1.self_attn\n",
            "model.layers.1.self_attn.q_proj\n",
            "model.layers.1.self_attn.k_proj\n",
            "model.layers.1.self_attn.v_proj\n",
            "model.layers.1.self_attn.o_proj\n",
            "model.layers.1.mlp\n",
            "model.layers.1.mlp.gate_proj\n",
            "model.layers.1.mlp.up_proj\n",
            "model.layers.1.mlp.down_proj\n",
            "model.layers.1.mlp.act_fn\n",
            "model.layers.1.input_layernorm\n",
            "model.layers.1.post_attention_layernorm\n",
            "model.layers.2\n",
            "model.layers.2.self_attn\n",
            "model.layers.2.self_attn.q_proj\n",
            "model.layers.2.self_attn.k_proj\n",
            "model.layers.2.self_attn.v_proj\n",
            "model.layers.2.self_attn.o_proj\n",
            "model.layers.2.mlp\n",
            "model.layers.2.mlp.gate_proj\n",
            "model.layers.2.mlp.up_proj\n",
            "model.layers.2.mlp.down_proj\n",
            "model.layers.2.mlp.act_fn\n",
            "model.layers.2.input_layernorm\n",
            "model.layers.2.post_attention_layernorm\n",
            "model.layers.3\n",
            "model.layers.3.self_attn\n",
            "model.layers.3.self_attn.q_proj\n",
            "model.layers.3.self_attn.k_proj\n",
            "model.layers.3.self_attn.v_proj\n",
            "model.layers.3.self_attn.o_proj\n",
            "model.layers.3.mlp\n",
            "model.layers.3.mlp.gate_proj\n",
            "model.layers.3.mlp.up_proj\n",
            "model.layers.3.mlp.down_proj\n",
            "model.layers.3.mlp.act_fn\n",
            "model.layers.3.input_layernorm\n",
            "model.layers.3.post_attention_layernorm\n",
            "model.layers.4\n",
            "model.layers.4.self_attn\n",
            "model.layers.4.self_attn.q_proj\n",
            "model.layers.4.self_attn.k_proj\n",
            "model.layers.4.self_attn.v_proj\n",
            "model.layers.4.self_attn.o_proj\n",
            "model.layers.4.mlp\n",
            "model.layers.4.mlp.gate_proj\n",
            "model.layers.4.mlp.up_proj\n",
            "model.layers.4.mlp.down_proj\n",
            "model.layers.4.mlp.act_fn\n",
            "model.layers.4.input_layernorm\n",
            "model.layers.4.post_attention_layernorm\n",
            "model.layers.5\n",
            "model.layers.5.self_attn\n",
            "model.layers.5.self_attn.q_proj\n",
            "model.layers.5.self_attn.k_proj\n",
            "model.layers.5.self_attn.v_proj\n",
            "model.layers.5.self_attn.o_proj\n",
            "model.layers.5.mlp\n",
            "model.layers.5.mlp.gate_proj\n",
            "model.layers.5.mlp.up_proj\n",
            "model.layers.5.mlp.down_proj\n",
            "model.layers.5.mlp.act_fn\n",
            "model.layers.5.input_layernorm\n",
            "model.layers.5.post_attention_layernorm\n",
            "model.layers.6\n",
            "model.layers.6.self_attn\n",
            "model.layers.6.self_attn.q_proj\n",
            "model.layers.6.self_attn.k_proj\n",
            "model.layers.6.self_attn.v_proj\n",
            "model.layers.6.self_attn.o_proj\n",
            "model.layers.6.mlp\n",
            "model.layers.6.mlp.gate_proj\n",
            "model.layers.6.mlp.up_proj\n",
            "model.layers.6.mlp.down_proj\n",
            "model.layers.6.mlp.act_fn\n",
            "model.layers.6.input_layernorm\n",
            "model.layers.6.post_attention_layernorm\n",
            "model.layers.7\n",
            "model.layers.7.self_attn\n",
            "model.layers.7.self_attn.q_proj\n",
            "model.layers.7.self_attn.k_proj\n",
            "model.layers.7.self_attn.v_proj\n",
            "model.layers.7.self_attn.o_proj\n",
            "model.layers.7.mlp\n",
            "model.layers.7.mlp.gate_proj\n",
            "model.layers.7.mlp.up_proj\n",
            "model.layers.7.mlp.down_proj\n",
            "model.layers.7.mlp.act_fn\n",
            "model.layers.7.input_layernorm\n",
            "model.layers.7.post_attention_layernorm\n",
            "model.layers.8\n",
            "model.layers.8.self_attn\n",
            "model.layers.8.self_attn.q_proj\n",
            "model.layers.8.self_attn.k_proj\n",
            "model.layers.8.self_attn.v_proj\n",
            "model.layers.8.self_attn.o_proj\n",
            "model.layers.8.mlp\n",
            "model.layers.8.mlp.gate_proj\n",
            "model.layers.8.mlp.up_proj\n",
            "model.layers.8.mlp.down_proj\n",
            "model.layers.8.mlp.act_fn\n",
            "model.layers.8.input_layernorm\n",
            "model.layers.8.post_attention_layernorm\n",
            "model.layers.9\n",
            "model.layers.9.self_attn\n",
            "model.layers.9.self_attn.q_proj\n",
            "model.layers.9.self_attn.k_proj\n",
            "model.layers.9.self_attn.v_proj\n",
            "model.layers.9.self_attn.o_proj\n",
            "model.layers.9.mlp\n",
            "model.layers.9.mlp.gate_proj\n",
            "model.layers.9.mlp.up_proj\n",
            "model.layers.9.mlp.down_proj\n",
            "model.layers.9.mlp.act_fn\n",
            "model.layers.9.input_layernorm\n",
            "model.layers.9.post_attention_layernorm\n",
            "model.layers.10\n",
            "model.layers.10.self_attn\n",
            "model.layers.10.self_attn.q_proj\n",
            "model.layers.10.self_attn.k_proj\n",
            "model.layers.10.self_attn.v_proj\n",
            "model.layers.10.self_attn.o_proj\n",
            "model.layers.10.mlp\n",
            "model.layers.10.mlp.gate_proj\n",
            "model.layers.10.mlp.up_proj\n",
            "model.layers.10.mlp.down_proj\n",
            "model.layers.10.mlp.act_fn\n",
            "model.layers.10.input_layernorm\n",
            "model.layers.10.post_attention_layernorm\n",
            "model.layers.11\n",
            "model.layers.11.self_attn\n",
            "model.layers.11.self_attn.q_proj\n",
            "model.layers.11.self_attn.k_proj\n",
            "model.layers.11.self_attn.v_proj\n",
            "model.layers.11.self_attn.o_proj\n",
            "model.layers.11.mlp\n",
            "model.layers.11.mlp.gate_proj\n",
            "model.layers.11.mlp.up_proj\n",
            "model.layers.11.mlp.down_proj\n",
            "model.layers.11.mlp.act_fn\n",
            "model.layers.11.input_layernorm\n",
            "model.layers.11.post_attention_layernorm\n",
            "model.layers.12\n",
            "model.layers.12.self_attn\n",
            "model.layers.12.self_attn.q_proj\n",
            "model.layers.12.self_attn.k_proj\n",
            "model.layers.12.self_attn.v_proj\n",
            "model.layers.12.self_attn.o_proj\n",
            "model.layers.12.mlp\n",
            "model.layers.12.mlp.gate_proj\n",
            "model.layers.12.mlp.up_proj\n",
            "model.layers.12.mlp.down_proj\n",
            "model.layers.12.mlp.act_fn\n",
            "model.layers.12.input_layernorm\n",
            "model.layers.12.post_attention_layernorm\n",
            "model.layers.13\n",
            "model.layers.13.self_attn\n",
            "model.layers.13.self_attn.q_proj\n",
            "model.layers.13.self_attn.k_proj\n",
            "model.layers.13.self_attn.v_proj\n",
            "model.layers.13.self_attn.o_proj\n",
            "model.layers.13.mlp\n",
            "model.layers.13.mlp.gate_proj\n",
            "model.layers.13.mlp.up_proj\n",
            "model.layers.13.mlp.down_proj\n",
            "model.layers.13.mlp.act_fn\n",
            "model.layers.13.input_layernorm\n",
            "model.layers.13.post_attention_layernorm\n",
            "model.layers.14\n",
            "model.layers.14.self_attn\n",
            "model.layers.14.self_attn.q_proj\n",
            "model.layers.14.self_attn.k_proj\n",
            "model.layers.14.self_attn.v_proj\n",
            "model.layers.14.self_attn.o_proj\n",
            "model.layers.14.mlp\n",
            "model.layers.14.mlp.gate_proj\n",
            "model.layers.14.mlp.up_proj\n",
            "model.layers.14.mlp.down_proj\n",
            "model.layers.14.mlp.act_fn\n",
            "model.layers.14.input_layernorm\n",
            "model.layers.14.post_attention_layernorm\n",
            "model.layers.15\n",
            "model.layers.15.self_attn\n",
            "model.layers.15.self_attn.q_proj\n",
            "model.layers.15.self_attn.k_proj\n",
            "model.layers.15.self_attn.v_proj\n",
            "model.layers.15.self_attn.o_proj\n",
            "model.layers.15.mlp\n",
            "model.layers.15.mlp.gate_proj\n",
            "model.layers.15.mlp.up_proj\n",
            "model.layers.15.mlp.down_proj\n",
            "model.layers.15.mlp.act_fn\n",
            "model.layers.15.input_layernorm\n",
            "model.layers.15.post_attention_layernorm\n",
            "model.layers.16\n",
            "model.layers.16.self_attn\n",
            "model.layers.16.self_attn.q_proj\n",
            "model.layers.16.self_attn.k_proj\n",
            "model.layers.16.self_attn.v_proj\n",
            "model.layers.16.self_attn.o_proj\n",
            "model.layers.16.mlp\n",
            "model.layers.16.mlp.gate_proj\n",
            "model.layers.16.mlp.up_proj\n",
            "model.layers.16.mlp.down_proj\n",
            "model.layers.16.mlp.act_fn\n",
            "model.layers.16.input_layernorm\n",
            "model.layers.16.post_attention_layernorm\n",
            "model.layers.17\n",
            "model.layers.17.self_attn\n",
            "model.layers.17.self_attn.q_proj\n",
            "model.layers.17.self_attn.k_proj\n",
            "model.layers.17.self_attn.v_proj\n",
            "model.layers.17.self_attn.o_proj\n",
            "model.layers.17.mlp\n",
            "model.layers.17.mlp.gate_proj\n",
            "model.layers.17.mlp.up_proj\n",
            "model.layers.17.mlp.down_proj\n",
            "model.layers.17.mlp.act_fn\n",
            "model.layers.17.input_layernorm\n",
            "model.layers.17.post_attention_layernorm\n",
            "model.layers.18\n",
            "model.layers.18.self_attn\n",
            "model.layers.18.self_attn.q_proj\n",
            "model.layers.18.self_attn.k_proj\n",
            "model.layers.18.self_attn.v_proj\n",
            "model.layers.18.self_attn.o_proj\n",
            "model.layers.18.mlp\n",
            "model.layers.18.mlp.gate_proj\n",
            "model.layers.18.mlp.up_proj\n",
            "model.layers.18.mlp.down_proj\n",
            "model.layers.18.mlp.act_fn\n",
            "model.layers.18.input_layernorm\n",
            "model.layers.18.post_attention_layernorm\n",
            "model.layers.19\n",
            "model.layers.19.self_attn\n",
            "model.layers.19.self_attn.q_proj\n",
            "model.layers.19.self_attn.k_proj\n",
            "model.layers.19.self_attn.v_proj\n",
            "model.layers.19.self_attn.o_proj\n",
            "model.layers.19.mlp\n",
            "model.layers.19.mlp.gate_proj\n",
            "model.layers.19.mlp.up_proj\n",
            "model.layers.19.mlp.down_proj\n",
            "model.layers.19.mlp.act_fn\n",
            "model.layers.19.input_layernorm\n",
            "model.layers.19.post_attention_layernorm\n",
            "model.layers.20\n",
            "model.layers.20.self_attn\n",
            "model.layers.20.self_attn.q_proj\n",
            "model.layers.20.self_attn.k_proj\n",
            "model.layers.20.self_attn.v_proj\n",
            "model.layers.20.self_attn.o_proj\n",
            "model.layers.20.mlp\n",
            "model.layers.20.mlp.gate_proj\n",
            "model.layers.20.mlp.up_proj\n",
            "model.layers.20.mlp.down_proj\n",
            "model.layers.20.mlp.act_fn\n",
            "model.layers.20.input_layernorm\n",
            "model.layers.20.post_attention_layernorm\n",
            "model.layers.21\n",
            "model.layers.21.self_attn\n",
            "model.layers.21.self_attn.q_proj\n",
            "model.layers.21.self_attn.k_proj\n",
            "model.layers.21.self_attn.v_proj\n",
            "model.layers.21.self_attn.o_proj\n",
            "model.layers.21.mlp\n",
            "model.layers.21.mlp.gate_proj\n",
            "model.layers.21.mlp.up_proj\n",
            "model.layers.21.mlp.down_proj\n",
            "model.layers.21.mlp.act_fn\n",
            "model.layers.21.input_layernorm\n",
            "model.layers.21.post_attention_layernorm\n",
            "model.layers.22\n",
            "model.layers.22.self_attn\n",
            "model.layers.22.self_attn.q_proj\n",
            "model.layers.22.self_attn.k_proj\n",
            "model.layers.22.self_attn.v_proj\n",
            "model.layers.22.self_attn.o_proj\n",
            "model.layers.22.mlp\n",
            "model.layers.22.mlp.gate_proj\n",
            "model.layers.22.mlp.up_proj\n",
            "model.layers.22.mlp.down_proj\n",
            "model.layers.22.mlp.act_fn\n",
            "model.layers.22.input_layernorm\n",
            "model.layers.22.post_attention_layernorm\n",
            "model.layers.23\n",
            "model.layers.23.self_attn\n",
            "model.layers.23.self_attn.q_proj\n",
            "model.layers.23.self_attn.k_proj\n",
            "model.layers.23.self_attn.v_proj\n",
            "model.layers.23.self_attn.o_proj\n",
            "model.layers.23.mlp\n",
            "model.layers.23.mlp.gate_proj\n",
            "model.layers.23.mlp.up_proj\n",
            "model.layers.23.mlp.down_proj\n",
            "model.layers.23.mlp.act_fn\n",
            "model.layers.23.input_layernorm\n",
            "model.layers.23.post_attention_layernorm\n",
            "model.layers.24\n",
            "model.layers.24.self_attn\n",
            "model.layers.24.self_attn.q_proj\n",
            "model.layers.24.self_attn.k_proj\n",
            "model.layers.24.self_attn.v_proj\n",
            "model.layers.24.self_attn.o_proj\n",
            "model.layers.24.mlp\n",
            "model.layers.24.mlp.gate_proj\n",
            "model.layers.24.mlp.up_proj\n",
            "model.layers.24.mlp.down_proj\n",
            "model.layers.24.mlp.act_fn\n",
            "model.layers.24.input_layernorm\n",
            "model.layers.24.post_attention_layernorm\n",
            "model.layers.25\n",
            "model.layers.25.self_attn\n",
            "model.layers.25.self_attn.q_proj\n",
            "model.layers.25.self_attn.k_proj\n",
            "model.layers.25.self_attn.v_proj\n",
            "model.layers.25.self_attn.o_proj\n",
            "model.layers.25.mlp\n",
            "model.layers.25.mlp.gate_proj\n",
            "model.layers.25.mlp.up_proj\n",
            "model.layers.25.mlp.down_proj\n",
            "model.layers.25.mlp.act_fn\n",
            "model.layers.25.input_layernorm\n",
            "model.layers.25.post_attention_layernorm\n",
            "model.layers.26\n",
            "model.layers.26.self_attn\n",
            "model.layers.26.self_attn.q_proj\n",
            "model.layers.26.self_attn.k_proj\n",
            "model.layers.26.self_attn.v_proj\n",
            "model.layers.26.self_attn.o_proj\n",
            "model.layers.26.mlp\n",
            "model.layers.26.mlp.gate_proj\n",
            "model.layers.26.mlp.up_proj\n",
            "model.layers.26.mlp.down_proj\n",
            "model.layers.26.mlp.act_fn\n",
            "model.layers.26.input_layernorm\n",
            "model.layers.26.post_attention_layernorm\n",
            "model.layers.27\n",
            "model.layers.27.self_attn\n",
            "model.layers.27.self_attn.q_proj\n",
            "model.layers.27.self_attn.k_proj\n",
            "model.layers.27.self_attn.v_proj\n",
            "model.layers.27.self_attn.o_proj\n",
            "model.layers.27.mlp\n",
            "model.layers.27.mlp.gate_proj\n",
            "model.layers.27.mlp.up_proj\n",
            "model.layers.27.mlp.down_proj\n",
            "model.layers.27.mlp.act_fn\n",
            "model.layers.27.input_layernorm\n",
            "model.layers.27.post_attention_layernorm\n",
            "model.layers.28\n",
            "model.layers.28.self_attn\n",
            "model.layers.28.self_attn.q_proj\n",
            "model.layers.28.self_attn.k_proj\n",
            "model.layers.28.self_attn.v_proj\n",
            "model.layers.28.self_attn.o_proj\n",
            "model.layers.28.mlp\n",
            "model.layers.28.mlp.gate_proj\n",
            "model.layers.28.mlp.up_proj\n",
            "model.layers.28.mlp.down_proj\n",
            "model.layers.28.mlp.act_fn\n",
            "model.layers.28.input_layernorm\n",
            "model.layers.28.post_attention_layernorm\n",
            "model.layers.29\n",
            "model.layers.29.self_attn\n",
            "model.layers.29.self_attn.q_proj\n",
            "model.layers.29.self_attn.k_proj\n",
            "model.layers.29.self_attn.v_proj\n",
            "model.layers.29.self_attn.o_proj\n",
            "model.layers.29.mlp\n",
            "model.layers.29.mlp.gate_proj\n",
            "model.layers.29.mlp.up_proj\n",
            "model.layers.29.mlp.down_proj\n",
            "model.layers.29.mlp.act_fn\n",
            "model.layers.29.input_layernorm\n",
            "model.layers.29.post_attention_layernorm\n",
            "model.layers.30\n",
            "model.layers.30.self_attn\n",
            "model.layers.30.self_attn.q_proj\n",
            "model.layers.30.self_attn.k_proj\n",
            "model.layers.30.self_attn.v_proj\n",
            "model.layers.30.self_attn.o_proj\n",
            "model.layers.30.mlp\n",
            "model.layers.30.mlp.gate_proj\n",
            "model.layers.30.mlp.up_proj\n",
            "model.layers.30.mlp.down_proj\n",
            "model.layers.30.mlp.act_fn\n",
            "model.layers.30.input_layernorm\n",
            "model.layers.30.post_attention_layernorm\n",
            "model.layers.31\n",
            "model.layers.31.self_attn\n",
            "model.layers.31.self_attn.q_proj\n",
            "model.layers.31.self_attn.k_proj\n",
            "model.layers.31.self_attn.v_proj\n",
            "model.layers.31.self_attn.o_proj\n",
            "model.layers.31.mlp\n",
            "model.layers.31.mlp.gate_proj\n",
            "model.layers.31.mlp.up_proj\n",
            "model.layers.31.mlp.down_proj\n",
            "model.layers.31.mlp.act_fn\n",
            "model.layers.31.input_layernorm\n",
            "model.layers.31.post_attention_layernorm\n",
            "model.norm\n",
            "model.rotary_emb\n",
            "lm_head\n"
          ]
        }
      ],
      "source": [
        "# Show layers\n",
        "for name, module in model.named_modules():\n",
        "    print(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NYNuGU_wcHOY",
        "outputId": "a7d33839-f16b-490a-e2e3-dcc2fd8725cb"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output directory: /content/drive/MyDrive/Models/meta-llama/CodeLlama-7b-Python-hf_run230867fa23ea\n",
            "MLflow Tracking UI: https://b627-35-187-235-63.ngrok-free.app\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbernahrndt\u001b[0m (\u001b[33mteam-mbernahr\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.4"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250121_170214-7ia12959</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/team-mbernahr/DocstringGenerator/runs/7ia12959' target=\"_blank\">run_230867fa23ea</a></strong> to <a href='https://wandb.ai/team-mbernahr/DocstringGenerator' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/team-mbernahr/DocstringGenerator' target=\"_blank\">https://wandb.ai/team-mbernahr/DocstringGenerator</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/team-mbernahr/DocstringGenerator/runs/7ia12959' target=\"_blank\">https://wandb.ai/team-mbernahr/DocstringGenerator/runs/7ia12959</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1001' max='1200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1001/1200 7:47:39 < 1:33:09, 0.04 it/s, Epoch 0.64/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.866200</td>\n",
              "      <td>0.740692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.876600</td>\n",
              "      <td>0.720755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.759100</td>\n",
              "      <td>0.710783</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.803200</td>\n",
              "      <td>0.704794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.713800</td>\n",
              "      <td>0.701615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.776300</td>\n",
              "      <td>0.697092</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.835300</td>\n",
              "      <td>0.694409</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.763100</td>\n",
              "      <td>0.691541</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.809700</td>\n",
              "      <td>0.692076</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "    <div>\n",
              "      \n",
              "      <progress value='377' max='502' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [377/502 26:22 < 08:46, 0.24 it/s]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1200' max='1200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1200/1200 10:02:27, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.866200</td>\n",
              "      <td>0.740692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.876600</td>\n",
              "      <td>0.720755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.759100</td>\n",
              "      <td>0.710783</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.803200</td>\n",
              "      <td>0.704794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.713800</td>\n",
              "      <td>0.701615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.776300</td>\n",
              "      <td>0.697092</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.835300</td>\n",
              "      <td>0.694409</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.763100</td>\n",
              "      <td>0.691541</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.809700</td>\n",
              "      <td>0.692076</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.809200</td>\n",
              "      <td>0.688078</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.852800</td>\n",
              "      <td>0.686246</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.782900</td>\n",
              "      <td>0.684579</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▆▄▄▃▃▂▂▂▁▁▁</td></tr><tr><td>eval/runtime</td><td>▅█▄▃▇▇▇▅▅▇▅▁</td></tr><tr><td>eval/samples_per_second</td><td>▄▁▅▆▂▂▂▄▄▂▄█</td></tr><tr><td>eval/steps_per_second</td><td>▄▁▅▅▁▁▁▄▄▁▄█</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇█</td></tr><tr><td>train/grad_norm</td><td>▁▁█▃▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>▇█▆▅▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>█▇▃▇▆▇▂▆▃▇▆▆▆▆▂█▄▅▂▄▄▆▄▇▁▇▅▅▆▁▅▂▂▇▄▄▇▂▃▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.68458</td></tr><tr><td>eval/runtime</td><td>2028.704</td></tr><tr><td>eval/samples_per_second</td><td>1.979</td></tr><tr><td>eval/steps_per_second</td><td>0.247</td></tr><tr><td>total_flos</td><td>1.7105050809903514e+17</td></tr><tr><td>train/epoch</td><td>0.77121</td></tr><tr><td>train/global_step</td><td>1200</td></tr><tr><td>train/grad_norm</td><td>4.45961</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/loss</td><td>0.7829</td></tr><tr><td>train_loss</td><td>0.7075</td></tr><tr><td>train_runtime</td><td>36254.1091</td></tr><tr><td>train_samples_per_second</td><td>0.199</td></tr><tr><td>train_steps_per_second</td><td>0.033</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">run_230867fa23ea</strong> at: <a href='https://wandb.ai/team-mbernahr/DocstringGenerator/runs/7ia12959' target=\"_blank\">https://wandb.ai/team-mbernahr/DocstringGenerator/runs/7ia12959</a><br> View project at: <a href='https://wandb.ai/team-mbernahr/DocstringGenerator' target=\"_blank\">https://wandb.ai/team-mbernahr/DocstringGenerator</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250121_170214-7ia12959/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Fine-tuning configuration\n",
        "model_name = model_to_finetune\n",
        "\n",
        "# Generate a random run ID\n",
        "current_time = str(time.time()).encode('utf-8')\n",
        "hash_object = hashlib.sha256(current_time)\n",
        "hex_digest = hash_object.hexdigest()\n",
        "random_string = hex_digest[:12]\n",
        "run_id = random_string\n",
        "\n",
        "# LoRA parameters\n",
        "lora_r = 32\n",
        "lora_alpha = 64\n",
        "lora_dropout = 0.1\n",
        "\n",
        "# Training parameter\n",
        "num_train_epochs = 3\n",
        "per_device_train_batch_size = 3\n",
        "per_device_eval_batch_size = 6\n",
        "gradient_accumulation_steps = 2\n",
        "gradient_checkpointing = True\n",
        "max_grad_norm = 0.5\n",
        "learning_rate = 0.00007\n",
        "weight_decay = 0.015\n",
        "optim = \"paged_adamw_32bit\"\n",
        "lr_scheduler_type = \"inverse_sqrt\"\n",
        "max_steps = 1200\n",
        "warmup_ratio = 0.03\n",
        "group_by_length = True\n",
        "save_steps = 100\n",
        "logging_steps = 10\n",
        "eval_steps = 100\n",
        "\n",
        "# Output directory\n",
        "run_name = f\"{model_name}_run{run_id}\"\n",
        "output_dir = os.path.join(base_output_dir, run_name)\n",
        "print(f\"Output directory: {output_dir}\")\n",
        "\n",
        "# Fine-tuned model name\n",
        "new_model = os.path.join(output_dir, \"end_of_training\")\n",
        "\n",
        "# Target modules to adapt key components to the model type (Falcon / CodeLlama):\n",
        "# - Attention Projections: Query, Key, Value, and Output\n",
        "# - Feed-Forward Network: Input (Expansion) and Output (Reduction)\n",
        "# - Embedding Matrix: Maps tokens to dense vectors\n",
        "if \"CodeLlama\" in model_name:\n",
        "    target_modules = [\n",
        "        \"self_attn.q_proj\",\n",
        "        \"self_attn.k_proj\",\n",
        "        \"self_attn.v_proj\",\n",
        "        \"self_attn.o_proj\",\n",
        "        \"mlp.gate_proj\",\n",
        "        \"mlp.down_proj\",\n",
        "        \"embed_tokens\",\n",
        "    ]\n",
        "elif \"falcon\" in model_name:\n",
        "    target_modules = [\n",
        "        \"self_attention.query_key_value\",\n",
        "        \"self_attention.dense\",\n",
        "        \"mlp.dense_h_to_4h\",\n",
        "        \"mlp.dense_4h_to_h\",\n",
        "        \"word_embeddings\",\n",
        "    ]\n",
        "else:\n",
        "    target_modules = None\n",
        "\n",
        "# Load LoRA configuration\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    r=lora_r,\n",
        "    bias=\"all\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=target_modules,\n",
        ")\n",
        "\n",
        "\n",
        "# Monitoring\n",
        "projectname='DocstringGenerator'\n",
        "\n",
        "# Initialize MLflow\n",
        "if on_colab:\n",
        "    # Set ngrok API token\n",
        "    from pyngrok import ngrok\n",
        "    ngrok.set_auth_token(ngrok_api_token)\n",
        "    # Starts MLflow UI in the background\n",
        "    get_ipython().system_raw(\"mlflow ui --backend-store-uri file:/content/mlruns --port 5000 &\")\n",
        "    # Forward port 5000 via ngrok\n",
        "    public_url = ngrok.connect(5000)\n",
        "    print(\"MLflow Tracking UI:\", public_url.public_url)\n",
        "    mlflow.set_tracking_uri(\"file:/content/mlruns\")\n",
        "else:\n",
        "    # run in terminal:\n",
        "    # mlflow server --host 127.0.0.1 --port 8080\n",
        "    mlflow.set_tracking_uri(\"http://127.0.0.1:8080\")\n",
        "\n",
        "mlflow.set_experiment(projectname)\n",
        "mlflow.start_run(run_name=f\"run_{run_id}\")\n",
        "\n",
        "# Initialize Weights & Biases\n",
        "wandb.login(key=wandb_api_token)\n",
        "wandb.init(\n",
        "    project=projectname,\n",
        "    name=f\"run_{run_id}\",\n",
        "    config={\n",
        "    \"lora_r\":lora_r,\n",
        "    \"lora_dropout\":lora_dropout,\n",
        "    \"learning_rate\": learning_rate,\n",
        "    \"num_train_epochs\": num_train_epochs,\n",
        "    }\n",
        ")\n",
        "\n",
        "# Set training parameters\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    optim=optim,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    max_steps=max_steps,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    group_by_length=group_by_length,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=eval_steps,\n",
        "    gradient_checkpointing=gradient_checkpointing,\n",
        "    report_to=[\"wandb\", \"mlflow\"],\n",
        "    run_name=run_id,\n",
        "    logging_dir=os.path.join(base_output_dir, \"Results/runs/\", run_name),\n",
        ")\n",
        "\n",
        "\n",
        "# Initialize the SFT Trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=tokenized_datasets[\"train\"].shuffle(),\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    peft_config=peft_config,\n",
        "    # dataset_text_field=\"text\",\n",
        "    # max_seq_length=None,\n",
        "    processing_class=tokenizer,\n",
        "    args=training_arguments,\n",
        "    # packing=False,\n",
        ")\n",
        "\n",
        "# Pre-process the model of layer norm for stable training\n",
        "for name, module in trainer.model.named_modules():\n",
        "    if \"norm\" in name:\n",
        "        module = module.to(torch.float32)\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Log model metrics to MLflow\n",
        "if trainer.state.log_history:\n",
        "    metrics = trainer.state.log_history[-1]\n",
        "    for k, v in metrics.items():\n",
        "        if isinstance(v, (int, float)):\n",
        "            mlflow.log_metric(k, v)\n",
        "\n",
        "# Save the trained model\n",
        "trainer.model.save_pretrained(new_model)\n",
        "mlflow.log_artifacts(output_dir)\n",
        "\n",
        "# End MLflow and W&B session\n",
        "mlflow.end_run()\n",
        "wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b68264a60c984614aa4b6b6de2a31e7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d92bb162dcb74c149f54db3d035d2805",
              "IPY_MODEL_62f0778cf823447da8e80cb56268701e",
              "IPY_MODEL_7eb1158c75484a418bfcbe52764cb7e7"
            ],
            "layout": "IPY_MODEL_d7b75f2d30cd49d7b3e5c982ea9de65c"
          }
        },
        "d92bb162dcb74c149f54db3d035d2805": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f29c0059b57c4084b18c4089b97873ad",
            "placeholder": "​",
            "style": "IPY_MODEL_74f190367ca649b38c8ea3fbdd2ff93e",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "62f0778cf823447da8e80cb56268701e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95e63e62a84c482081243766b58f88d7",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_77915f031ee8435c945bc06da6da0256",
            "value": 2
          }
        },
        "7eb1158c75484a418bfcbe52764cb7e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7114f9d786b54139981caec48b3a0b38",
            "placeholder": "​",
            "style": "IPY_MODEL_50ffacca6cdc4e9285c226b703ff988e",
            "value": " 2/2 [00:09&lt;00:00,  4.18s/it]"
          }
        },
        "d7b75f2d30cd49d7b3e5c982ea9de65c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f29c0059b57c4084b18c4089b97873ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74f190367ca649b38c8ea3fbdd2ff93e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "95e63e62a84c482081243766b58f88d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77915f031ee8435c945bc06da6da0256": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7114f9d786b54139981caec48b3a0b38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50ffacca6cdc4e9285c226b703ff988e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}